# --- ã“ã“ã¯å…ˆé ­ã® import ã®ã™ãä¸‹ã«ç½®ã„ã¦OK ---
import streamlit as st
import pandas as pd
import unicodedata, re, difflib, os
from pathlib import Path

# ====== è¡¨ç¤ºç³»ï¼ˆã‚¹ãƒãƒ›æœ€é©åŒ–ï¼‹æ–‡è¨€ï¼‰ ======
st.set_page_config(page_title="å¯Œæ°¸é›»æ©Ÿ è£œåŠ©é‡‘ä»£è¡Œ", page_icon="ğŸ› ï¸", layout="wide")

# ç”»åƒï¼ˆãƒ’ãƒ¼ãƒ­ãƒ¼ï¼‰ã¯åˆ‡ã‚‰ã•ãšã«è¦‹ã›ã‚‹ãŸã‚ contain + æ¨ªã«ä½™ç™½
st.markdown("""
<style>
/* ãƒ’ãƒ¼ãƒ­ãƒ¼ãŒä¸‹ã®è¦ç´ ã®ã‚¯ãƒªãƒƒã‚¯ã‚’é‚ªé­”ã—ãªã„ã‚ˆã†ã« */
.hero-wrap,
.hero-copy { pointer-events: none; }

/* ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¯é–‰ã˜ã¦ã‚‹æ™‚ã¯ã‚¯ãƒªãƒƒã‚¯ç„¡åŠ¹ */
.menu-panel { opacity: 0; pointer-events: none; transition: opacity .2s; }
.menu-panel.open { opacity: 1; pointer-events: auto; }

/* ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯é€šå¸¸ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§OK */
.section { position: relative; z-index: 1; }

:root{ --fg:#0f172a; --fg2:#1f2937; --muted:#475569; --line:#e5e7eb;
       --brand:#ff8a00; --bg:#fafaf9; --radius:14px; }
html {font-size:16px;}
body { color:var(--fg2); background:var(--bg); line-height:1.75; }
.container{ max-width:980px; margin:0 auto; padding: 0 14px; }
h1{font-weight:800; color:var(--fg); margin:14px 0 10px; font-size:clamp(20px,5vw,28px); line-height:1.3}
h2{font-weight:700; color:var(--fg); margin:20px 0 12px; font-size:clamp(18px,4.5vw,22px); line-height:1.35}
p{margin:0 0 0.9rem}

.hero-wrap{ margin:10px 0 16px; }
.hero {
  border-radius: var(--radius);
  overflow: hidden;
  background:#fff;
  box-shadow: 0 6px 22px rgba(0,0,0,.08);
  padding: 0 8px; /* â† æ¨ªã«ä½™ç™½: ã‚¹ãƒãƒ›ã§åˆ‡ã‚Œã«ããã™ã‚‹ */
}
.hero img{
  width:100%;
  height:auto;
  border-radius: var(--radius);
  object-fit: contain;        /* â† åˆ‡ã‚‰ãšã«è¡¨ç¤º */
  aspect-ratio: 16/7;         /* é«˜ã•ã‚’å®‰å®šã•ã›ã‚‹ */
  background: #fff;
}
.note{font-size:12px; color:var(--muted); margin-top:6px; text-align:center}
.btn-row{ display:grid; grid-template-columns: 1fr 1fr; gap:10px; margin: 10px 0 12px;}
.btn{
  display:inline-flex; align-items:center; justify-content:center;
  padding:12px 14px; border-radius:999px; font-weight:700; text-decoration:none;
  background:var(--brand); color:#fff; box-shadow:0 6px 16px rgba(255,138,0,.25)
}
.btn-outline{background:#fff; color:var(--brand); border:2px solid var(--brand)}
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="container">', unsafe_allow_html=True)
st.markdown("### å¯Œæ°¸é›»æ©Ÿ è£œåŠ©é‡‘ä»£è¡Œ")

# ãƒ’ãƒ¼ãƒ­ãƒ¼ç”»åƒï¼ˆãƒªãƒã‚¸ãƒˆãƒªã® JPG ã‚’å‚ç…§ï¼‰
RAW_HERO = "https://raw.githubusercontent.com/TominagaKota/Hozyokin_check-web/main/assets/tomydenki_hero.jpg"

st.markdown('<div class="hero-wrap"><div class="hero">', unsafe_allow_html=True)
st.image(RAW_HERO, use_column_width=True)
st.markdown('</div></div>', unsafe_allow_html=True)

st.markdown("#### è£œåŠ©é‡‘ã‚‚å¯Œæ°¸é›»æ©Ÿã«ãŠã¾ã‹ã›ï¼")

# ä¸Šéƒ¨ãƒœã‚¿ãƒ³
colA, colB = st.columns(2)
with colA:
    st.markdown('<a class="btn" href="#tokyo_check">æ±äº¬éƒ½è£œåŠ©é‡‘ã‚’ä»®åˆ¤å®š</a>', unsafe_allow_html=True)
with colB:
    st.markdown('<a class="btn btn-outline" href="#contact">ä»Šã™ãç›¸è«‡</a>', unsafe_allow_html=True)

st.markdown('</div>', unsafe_allow_html=True)  # .container ã‚’ã‚¯ãƒ­ãƒ¼ã‚º





# -----------------------
# è£œåŠ©é‡‘åˆ¤å®šé–¢æ•°
# -----------------------
def check_hojo(apf, kw, category, region_list, units):
    hojo_files = [
        "kuni_hojo_2025.json",
        "tokyo_hojo_2025.json",
        "edogawa_hojo_2025.json"
    ]
    results = []
    total_sum = 0

    for filename in hojo_files:
        if not os.path.exists(filename):
            continue

        with open(filename, "r", encoding="utf-8") as f:
            hojo = json.load(f)

        name = hojo["è£œåŠ©é‡‘å"]
        required_category = hojo["ã‚«ãƒ†ã‚´ãƒª"]
        target_region = hojo["å¯¾è±¡åœ°åŸŸ"]
        condition = hojo["æ¡ä»¶"]
        amount_per_unit = hojo["è£œåŠ©é‡‘é¡"]
        max_total = hojo["ä¸Šé™é¡"]
        is_multiple_allowed = hojo["é‡è¤‡å¯"]

        if (
            category in required_category and
            any(area in target_region for area in region_list) and
            apf >= condition["APF_min"] and
            condition["kW_range"][0] <= kw <= condition["kW_range"][1]
        ):
            total = min(amount_per_unit * units, max_total)
            results.append({
                "name": name,
                "è£œåŠ©é‡‘é¡": total,
                "é‡è¤‡å¯": "å¯èƒ½" if is_multiple_allowed else "ä¸å¯"
            })
            total_sum += total

    return results, total_sum

# -----------------------
# Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“
# -----------------------
# ====== æ±äº¬éƒ½ï¼šå‹ç•ª æ­£å¼åˆ¤å®š ======
st.markdown('<div id="tokyo_check" class="container"></div>', unsafe_allow_html=True)
st.header("æ±äº¬éƒ½ï¼šå‹ç•ªã§æ­£å¼åˆ¤å®šï¼ˆå®¶åº­ç”¨ï¼‰")

TOKYO_CSV_PATH = "assets/tokyo_models.csv"  # ã“ã“ã«CSVã‚’ç½®ã

def normalize_model(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = unicodedata.normalize("NFKC", s).upper()
    s = s.replace("â€“","-").replace("â€”","-").replace("âˆ’","-").replace("ãƒ¼","-")
    s = re.sub(r"\s+", "", s)
    # æ‹¬å¼§ã®ä¸­èº«ï¼ˆè‰²ç­‰ï¼‰ã‚’é™¤å»
    s = re.sub(r"[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]", "", s)
    # æœ«å°¾ã®è‰²è¨˜å·ã ã‘å®‰å…¨ã«é™¤å»ï¼ˆ-W/-C/-K/-N/-S/-P/-T/-B/-Hï¼‰
    s = re.sub(r"-(W|C|K|N|S|P|T|B|H)$", "", s)
    return s

@st.cache_data(show_spinner=False)
def load_tokyo_models():
    if not os.path.exists(TOKYO_CSV_PATH):
        return set(), pd.DataFrame()
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã¯è‡ªå‹•åˆ¤å®š â†’ ãƒ€ãƒ¡ãªã‚‰ cp932
    try:
        df = pd.read_csv(TOKYO_CSV_PATH)
    except UnicodeDecodeError:
        df = pd.read_csv(TOKYO_CSV_PATH, encoding="cp932")
    # å‹ç•ªã£ã½ã„åˆ—ã‚’æŠ½å‡ºï¼ˆåˆ—åã«ã€Œå‹ã€ã€Œç•ªã€ãŒå…¥ã‚‹ã‚‚ã®ã‚’å„ªå…ˆï¼‰
    cand_cols = [c for c in df.columns if ("å‹" in c) or ("ç•ª" in c)]
    if not cand_cols:
        cand_cols = df.columns.tolist()
    vals = pd.Series(dtype=str)
    for c in cand_cols:
        vals = pd.concat([vals, df[c].astype(str)], ignore_index=True)
    vals = vals.dropna().map(normalize_model)
    model_set = set([v for v in vals if v])
    return model_set, df

tokyo_set, tokyo_df = load_tokyo_models()

st.caption(f"ç™»éŒ²å‹ç•ªæ•°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰: {len(tokyo_set)} ä»¶")

user_input = st.text_area("åˆ¤å®šã—ãŸã„å‹ç•ªã‚’æ”¹è¡Œ or ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›", height=120, placeholder="ä¾‹ï¼‰\nRASâ€“AJ36G\nS22ZTES-W\n... ãªã©")
do_check = st.button("æ­£å¼åˆ¤å®šã™ã‚‹")

def split_models(s: str):
    if not s: return []
    parts = re.split(r"[\n,ã€/]+", s)
    return [p.strip() for p in parts if p.strip()]

if do_check:
    rows = []
    for raw in split_models(user_input):
        norm = normalize_model(raw)
        if not norm:
            continue
        exact = norm in tokyo_set
        near = difflib.get_close_matches(norm, list(tokyo_set), n=3, cutoff=0.72)
        rows.append({
            "å…¥åŠ›": raw,
            "æ­£è¦åŒ–": norm,
            "æ­£å¼åˆ¤å®š": "â—‹ï¼ˆãƒªã‚¹ãƒˆä¸€è‡´ï¼‰" if exact else "Ã—ï¼ˆæœªç™»éŒ²ï¼‰",
            "å€™è£œï¼ˆè¿‘ã„é †ï¼‰": " / ".join(near[:3]) if not exact and near else ""
        })
    if rows:
        st.dataframe(pd.DataFrame(rows), use_container_width=True)
    else:
        st.info("å‹ç•ªãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
